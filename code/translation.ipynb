{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model based on https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Dense, Embedding, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "from nltk import FreqDist\n",
    "from data_utils import read_file_to_lines, load_embedding_weights, split_to_buckets\n",
    "import pickle\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# TODO how to properly log\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data/\"\n",
    "LOG_FOLDER = \"logs/\"\n",
    "MODEL_FOLDER = \"model/\"\n",
    "MODEL_WEIGHTS = \"model_weights.h5\"\n",
    "MODEL_WEIGHTS_PATH = \"{}{}\".format(MODEL_FOLDER, MODEL_WEIGHTS)\n",
    "EMBEDDINGS_PATH = \"G:/Clouds/DPbigFiles/facebookVectors/facebookPretrained-wiki.cs.vec\"\n",
    "\n",
    "# Special vocabulary symbols\n",
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "\n",
    "# pad is zero, because default value in the matrices is zero (np.zeroes)\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "LATENT_DIM = 256\n",
    "NUM_SAMPLES = 1000\n",
    "MAX_VOCAB_SIZE = 15000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "USE_BUCKETS = False\n",
    "BUCKET_RANGE = 10 # HOW BIG\n",
    "REVERSE = True\n",
    "\n",
    "TRAINING_SET = \"anki_ces-eng\"\n",
    "TEST_SET = \"news-commentary-v9.cs-en\"\n",
    "INPUT_LANG = \"cs\"\n",
    "TARGET_LANG = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(X_lines, y_lines):\n",
    "    logger.info(\"tokenizing lines...\")\n",
    "    # TODO use tokenization from Moses so its same as for Moses baseline model\n",
    "    X_word_seq = [text_to_word_sequence(x) for x in X_lines]\n",
    "    y_word_seq = [[_GO] + text_to_word_sequence(y) + [_EOS] for y in y_lines]\n",
    "    \n",
    "    # Retrieving max sequence length for both source and target\n",
    "    X_max_seq_len = max(len(seq) for seq in X_word_seq)\n",
    "    y_max_seq_len = max(len(seq) for seq in y_word_seq)    \n",
    "    \n",
    "    logger.info(\"Max sequence length for inputs: {}\".format(X_max_seq_len))\n",
    "    logger.info(\"Max sequence length for targets: {}\".format(y_max_seq_len))      \n",
    "    \n",
    "    return X_word_seq, y_word_seq, X_max_seq_len, y_max_seq_len\n",
    "\n",
    "def insert_symbol_to_vocab(vocab, symbol, index):\n",
    "    '''\n",
    "    symbol can potentially (for instance as a result of tokenizing where _go and _eos are added to sequence)\n",
    "    be already part of vocabulary, but we want it to be on specific index\n",
    "    '''\n",
    "    \n",
    "    if symbol in vocab:\n",
    "        vocab.remove(symbol)\n",
    "    \n",
    "    vocab.insert(index, symbol)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def get_vocabularies(X_word_seq, y_word_seq, max_vocab_size):\n",
    "    logger.info(\"creating vocabularies...\")\n",
    "    # Creating the vocabulary set with the most common words\n",
    "    # TODO how many most common words to use?\n",
    "    dist = FreqDist(np.hstack(X_word_seq))\n",
    "    X_vocab = dist.most_common(max_vocab_size)\n",
    "    dist = FreqDist(np.hstack(y_word_seq))\n",
    "    y_vocab = dist.most_common(max_vocab_size)\n",
    "    \n",
    "    # Creating an array of words from the vocabulary set,\n",
    "    # we will use this array as index-to-word dictionary\n",
    "    X_ix_to_word = [word[0] for word in X_vocab]\n",
    "    # ADD special vocabulary symbols at the start \n",
    "    insert_symbol_to_vocab(X_ix_to_word, _PAD, PAD_ID)\n",
    "    insert_symbol_to_vocab(X_ix_to_word, _UNK, UNK_ID)\n",
    "    X_ix_to_word = {index:word for index, word in enumerate(X_ix_to_word)}\n",
    "    X_word_to_ix = {X_ix_to_word[ix]:ix for ix in X_ix_to_word}\n",
    "    # TODO how to use pretrained embedding with these custom symbols\n",
    "    # https://github.com/fchollet/keras/issues/6480\n",
    "    # https://github.com/fchollet/keras/issues/3325\n",
    "    X_vocab_len = len(X_ix_to_word)\n",
    "\n",
    "    y_ix_to_word = [word[0] for word in y_vocab]\n",
    "    insert_symbol_to_vocab(y_ix_to_word, _PAD, PAD_ID)\n",
    "    insert_symbol_to_vocab(y_ix_to_word, _GO, GO_ID)\n",
    "    insert_symbol_to_vocab(y_ix_to_word, _EOS, EOS_ID)\n",
    "    insert_symbol_to_vocab(y_ix_to_word, _UNK, UNK_ID)\n",
    "    y_ix_to_word = {index:word for index, word in enumerate(y_ix_to_word)}\n",
    "    y_word_to_ix = {y_ix_to_word[ix]:ix for ix in y_ix_to_word}\n",
    "    y_vocab_len = len(y_ix_to_word)\n",
    "    \n",
    "    result = (X_ix_to_word, X_word_to_ix, X_vocab_len,\n",
    "              y_ix_to_word, y_word_to_ix, y_vocab_len)\n",
    "    \n",
    "    logger.info(\"Number of samples: {}\".format(len(X_word_seq)))\n",
    "    logger.info(\"Number of input dictionary: {}\".format(X_vocab_len))\n",
    "    logger.info(\"Number of target dictionary: {}\".format(y_vocab_len))  \n",
    "    \n",
    "    return result\n",
    "\n",
    "def encode_sequences(X_word_seq, y_word_seq,\n",
    "                     X_max_seq_len, y_max_seq_len,\n",
    "                     X_word_to_ix, y_word_to_ix,\n",
    "                     reverse=True):\n",
    "    '''\n",
    "    Take word sequences and convert them so that the model can be fit with them.\n",
    "    Input words are just converted to integer index\n",
    "    Target words are encoded to one hot vectors of target vocabulary length\n",
    "    '''\n",
    "    logger.info(\"Encoding sequences...\")\n",
    "    \n",
    "    y_vocab_len = len(y_word_to_ix)\n",
    "    \n",
    "    encoder_input_data = np.zeros(\n",
    "        (len(X_word_seq), X_max_seq_len), dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(X_word_seq), y_max_seq_len, y_vocab_len),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(X_word_seq), y_max_seq_len, y_vocab_len),\n",
    "        dtype='float32')\n",
    "\n",
    "    # prepare source sentences for embedding layer (encode to indexes)\n",
    "    for i, seq in enumerate(X_word_seq):\n",
    "        if reverse: # for better results according to paper Sequence to seq...\n",
    "            seq = seq[::-1]\n",
    "        for t, word in enumerate(seq):\n",
    "            if word in X_word_to_ix:\n",
    "                encoder_input_data[i][t] = X_word_to_ix[word]\n",
    "            else:\n",
    "                encoder_input_data[i][t] = UNK_ID\n",
    "\n",
    "    # encode target sentences to one hot encoding            \n",
    "    for i, seq in enumerate(y_word_seq):\n",
    "        for t, word in enumerate(seq):\n",
    "            if word in y_word_to_ix:\n",
    "                index = y_word_to_ix[word]\n",
    "            else:\n",
    "                index = UNK_ID\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t][index] = 1\n",
    "\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, index] = 1\n",
    "    \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "\n",
    "def prepare_training_dataset(dataset_path, input_lang, output_lang, num_samples):    \n",
    "    X_file_path = \"{}.{}\".format(dataset_path, input_lang)\n",
    "    X_lines = read_file_to_lines(X_file_path, num_samples)\n",
    "    \n",
    "    y_file_path = \"{}.{}\".format(dataset_path, output_lang)\n",
    "    y_lines = read_file_to_lines(y_file_path, num_samples)    \n",
    "    \n",
    "    X_word_seq, y_word_seq, X_max_seq_len, y_max_seq_len = tokenize(X_lines, y_lines)\n",
    "    \n",
    "    X_ix_to_word, X_word_to_ix, X_vocab_len, y_ix_to_word, y_word_to_ix, y_vocab_len = get_vocabularies(X_word_seq, y_word_seq, MAX_VOCAB_SIZE)\n",
    "    \n",
    "\n",
    "    if USE_BUCKETS:\n",
    "        encoder_input_data = []\n",
    "        decoder_input_data = []\n",
    "        decoder_target_data = []\n",
    "\n",
    "        buckets = split_to_buckets(X_word_seq, y_word_seq, BUCKET_RANGE, X_max_seq_len, y_max_seq_len)\n",
    "        \n",
    "        for ix, bucket in buckets.items():\n",
    "            enc_in, dec_in, dec_tar = encode_sequences(\n",
    "                bucket[\"X_word_seq\"], bucket[\"y_word_seq\"],\n",
    "                bucket[\"X_max_seq_len\"], bucket[\"y_max_seq_len\"],\n",
    "                X_word_to_ix, y_word_to_ix,\n",
    "                reverse=REVERSE\n",
    "            )                                \n",
    "            \n",
    "            encoder_input_data.append(enc_in)\n",
    "            decoder_input_data.append(dec_in)\n",
    "            decoder_target_data.append(dec_tar)\n",
    "    else:\n",
    "        encoder_input_data, decoder_input_data, decoder_target_data = encode_sequences(\n",
    "            X_word_seq, y_word_seq,\n",
    "            X_max_seq_len, y_max_seq_len,\n",
    "            X_word_to_ix, y_word_to_ix,\n",
    "            reverse=REVERSE\n",
    "        )\n",
    "        \n",
    "        encoder_input_data = [encoder_input_data]\n",
    "        decoder_input_data = [decoder_input_data]\n",
    "        decoder_target_data = [decoder_target_data]\n",
    "\n",
    "    return {\n",
    "        \"X_word_seq\": X_word_seq, \"y_word_seq\": y_word_seq,\n",
    "        \"X_ix_to_word\": X_ix_to_word, \"X_word_to_ix\": X_word_to_ix,\n",
    "        \"X_vocab_len\": X_vocab_len, \"y_ix_to_word\": y_ix_to_word,\n",
    "        \"y_word_to_ix\": y_word_to_ix, \"y_vocab_len\": y_vocab_len,\n",
    "        \"encoder_input_data\": encoder_input_data,\n",
    "        \"decoder_input_data\": decoder_input_data,\n",
    "        \"decoder_target_data\": decoder_target_data\n",
    "    }\n",
    "\n",
    "def prepare_testing_dataset(dataset_path, input_lang, output_lang, num_samples,\n",
    "                           X_word_to_ix, y_word_to_ix, y_ix_to_word, y_vocab_len):    \n",
    "    X_file_path = \"{}.{}\".format(dataset_path, input_lang)\n",
    "    X_lines = read_file_to_lines(X_file_path, num_samples)\n",
    "    \n",
    "    y_file_path = \"{}.{}\".format(dataset_path, output_lang)\n",
    "    y_lines = read_file_to_lines(y_file_path, num_samples)    \n",
    "    \n",
    "    X_word_seq, y_word_seq, X_max_seq_len, y_max_seq_len = tokenize(X_lines, y_lines)\n",
    "\n",
    "    encoder_input_data, decoder_input_data, decoder_target_data = encode_sequences(\n",
    "        X_word_seq, y_word_seq,\n",
    "        X_max_seq_len, y_max_seq_len,\n",
    "        X_word_to_ix, y_word_to_ix,\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"y_ix_to_word\": y_ix_to_word,\n",
    "        \"y_vocab_len\": y_vocab_len,\n",
    "        \"y_max_seq_len\": y_max_seq_len,\n",
    "        \"encoder_input_data\": encoder_input_data,\n",
    "        \"decoder_input_data\": decoder_input_data,\n",
    "        \"decoder_target_data\": decoder_target_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_models(X_vocab_len, y_vocab_len,\n",
    "                  latent_dim, emmbedding_dim, embedding_weights=None):\n",
    "    logger.info(\"Creating models...\")\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,))\n",
    "    \n",
    "    if embedding_weights is not None:\n",
    "        embedding_weights = [embedding_weights] # Embedding layer wantes list as parameter\n",
    "    # TODO trainable False or True?\n",
    "    # TODO according to https://keras.io/layers/embeddings/\n",
    "    # input dim should be +1 when used with mask_zero..is it correctly set here?\n",
    "    embedding = Embedding(X_vocab_len, emmbedding_dim,\n",
    "                          weights=embedding_weights, mask_zero=True)\n",
    "    embedding_outputs = embedding(encoder_inputs)\n",
    "\n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(embedding_outputs)\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(None, y_vocab_len))\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(y_vocab_len, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "\n",
    "    # Next: inference mode (sampling).\n",
    "    # Here's the drill:\n",
    "    # 1) encode input and retrieve initial decoder state\n",
    "    # 2) run one step of decoder with this initial state\n",
    "    # and a \"start of sequence\" token as target.\n",
    "    # Output will be the next target token\n",
    "    # 3) Repeat with the current target token and current states\n",
    "\n",
    "    # Define sampling models\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "    decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model,\n",
    "                    y_ix_to_word, y_vocab_len, y_max_seq_len):    \n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, y_vocab_len))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, GO_ID] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1). # TODO ? can the batch size be bigger?\n",
    "    decoded_sentence = \"\"\n",
    "    while True:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = y_ix_to_word[sampled_token_index]\n",
    "        \n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.        \n",
    "        if sampled_word == _EOS:\n",
    "            break\n",
    "        \n",
    "        decoded_sentence += sampled_word + \" \"\n",
    "\n",
    "        if len(decoded_sentence) > y_max_seq_len:\n",
    "            break\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, y_vocab_len))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def encode_text_to_input_seq(text):\n",
    "    sequences = text_to_word_sequence(text)\n",
    "    x = np.zeros( (1, len(text)), dtype='float32')\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        if seq in X_word_to_ix:\n",
    "            ix = X_word_to_ix[seq]\n",
    "        else:\n",
    "            ix = UNK_ID\n",
    "        x[0][i] = ix\n",
    "    \n",
    "    return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-26 14:12:25,186 - data_utils - INFO - reading data/anki_ces-eng.cs into lines...\n",
      "2017-11-26 14:12:25,190 - data_utils - INFO - reading data/anki_ces-eng.en into lines...\n",
      "2017-11-26 14:12:25,194 - __main__ - INFO - tokenizing lines...\n",
      "2017-11-26 14:12:25,206 - __main__ - INFO - Max sequence length for inputs: 6\n",
      "2017-11-26 14:12:25,207 - __main__ - INFO - Max sequence length for targets: 7\n",
      "2017-11-26 14:12:25,208 - __main__ - INFO - creating vocabularies...\n",
      "2017-11-26 14:12:25,230 - __main__ - INFO - Number of samples: 1000\n",
      "2017-11-26 14:12:25,232 - __main__ - INFO - Number of input dictionary: 1035\n",
      "2017-11-26 14:12:25,233 - __main__ - INFO - Number of target dictionary: 679\n",
      "2017-11-26 14:12:25,235 - data_utils - INFO - splitting sequences to buckets with range 3\n",
      "2017-11-26 14:12:25,236 - data_utils - DEBUG - x_max_len = 6, y_max_len = 7\n",
      "2017-11-26 14:12:25,237 - data_utils - DEBUG - num buckets = 3\n",
      "2017-11-26 14:12:25,240 - __main__ - INFO - Encoding sequences...\n",
      "2017-11-26 14:12:25,242 - __main__ - INFO - Encoding sequences...\n",
      "2017-11-26 14:12:25,256 - __main__ - INFO - Encoding sequences...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    USE_BUCKETS = True\n",
    "    BUCKET_RANGE = 3\n",
    "    dataset_path = DATA_FOLDER + TRAINING_SET\n",
    "    training_dataset = prepare_training_dataset(\n",
    "        dataset_path,INPUT_LANG, TARGET_LANG, NUM_SAMPLES\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if os.path.isfile(MODEL_WEIGHTS_PATH):\n",
    "        embedding_weights = None # Will be loaded from file with all the model weights\n",
    "    else:\n",
    "        # load pretrained embeddings\n",
    "        embedding_weights = load_embedding_weights(EMBEDDINGS_PATH,\n",
    "                                                training_dataset[\"X_ix_to_word\"],\n",
    "                                                limit=MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-26 14:08:26,784 - __main__ - INFO - Creating models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 300)    310500      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, None, 679)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 256), (None, 570368      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 256),  958464      input_6[0][0]                    \n",
      "                                                                 lstm_3[0][1]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 679)    174503      lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,013,835\n",
      "Trainable params: 2,013,835\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "    model, encoder_model, decoder_model = define_models(\n",
    "        training_dataset[\"X_vocab_len\"],\n",
    "        training_dataset[\"y_vocab_len\"],\n",
    "        LATENT_DIM, EMBEDDING_DIM,\n",
    "        embedding_weights=embedding_weights\n",
    "    )     \n",
    "    \n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # logging for tensorboard\n",
    "    tensorboard = TensorBoard(log_dir=\"{}{}\".format(LOG_FOLDER, time()),\n",
    "                             write_graph=False)\n",
    "\n",
    "    # Run training\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                 metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-26 14:08:31,386 - __main__ - INFO - Loading model weights from file..\n",
      "2017-11-26 14:08:31,893 - __main__ - INFO - Epoch 1\n",
      "2017-11-26 14:08:31,894 - __main__ - INFO - Bucket 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4 samples, validate on 2 samples\n",
      "Epoch 1/1\n",
      "4/4 [==============================] - 1s 164ms/step - loss: 3.3944 - acc: 0.1667 - val_loss: 3.6136 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-26 14:08:35,651 - __main__ - INFO - Bucket 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 784 samples, validate on 196 samples\n",
      "Epoch 1/1\n",
      "784/784 [==============================] - 1s 2ms/step - loss: 2.5481 - acc: 0.2494 - val_loss: 3.7794 - val_acc: 0.2194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-26 14:08:36,927 - __main__ - INFO - Bucket 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples, validate on 3 samples\n",
      "Epoch 1/1\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 3.4708 - acc: 0.2078 - val_loss: 3.8201 - val_acc: 0.2381\n"
     ]
    }
   ],
   "source": [
    "    if os.path.isfile(MODEL_WEIGHTS_PATH):\n",
    "        logger.info(\"Loading model weights from file..\")\n",
    "        model.load_weights(MODEL_WEIGHTS_PATH)\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        logger.info(\"Epoch {}\".format(i + 1))\n",
    "        \n",
    "        for j in range(len(training_dataset[\"encoder_input_data\"])):  \n",
    "            if USE_BUCKETS:\n",
    "                logger.info(\"Bucket {}\".format(j))\n",
    "            \n",
    "            model.fit(\n",
    "                [\n",
    "                    training_dataset[\"encoder_input_data\"][j],\n",
    "                    training_dataset[\"decoder_input_data\"][j]\n",
    "                ],\n",
    "                training_dataset[\"decoder_target_data\"][j],\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=1,\n",
    "                validation_split=VALIDATION_SPLIT,\n",
    "                callbacks=[tensorboard]\n",
    "            )  \n",
    "        \n",
    "        model.save_weights(MODEL_WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # vocabularies of test dataset has to be the same as of training set\n",
    "    # otherwise embeddings would not correspond are use OOV\n",
    "    # and y one hot encodings wouldnt correspond either\n",
    "    dataset_path = DATA_FOLDER + TEST_SET\n",
    "    test_dataset = prepare_testing_dataset(\n",
    "        dataset_path,INPUT_LANG,\n",
    "        TARGET_LANG, NUM_SAMPLES,\n",
    "        training_dataset[\"X_word_to_ix\"],\n",
    "        training_dataset[\"y_word_to_ix\"],\n",
    "        training_dataset[\"y_ix_to_word\"],\n",
    "        training_dataset[\"y_vocab_len\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO probably create 4th model without decoder_input_data for evaluation?\n",
    "# maybe not\n",
    "model.evaluate(\n",
    "    [\n",
    "        test_dataset[\"encoder_input_data\"],\n",
    "        test_dataset[\"decoder_input_data\"]\n",
    "    ],\n",
    "    test_dataset[\"decoder_target_data\"],\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we translate test_dataset and compute resulting BLEU score?\n",
    "EVAL_TRANS = True \n",
    "if EVAL_TRANS:\n",
    "    logger.info(\"Translating test dataset for BLEU evaluation...\")\n",
    "    path = DATA_FOLDER + TEST_SET + \".\" + TARGET_LANG + \".translated\"\n",
    "    \n",
    "    with open(path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        for seq in test_dataset[\"encoder_input_data\"]:\n",
    "            decoded_sentence = decode_sequence(\n",
    "                seq, encoder_model, decoder_model, \n",
    "                test_dataset[\"y_ix_to_word\"],\n",
    "                test_dataset[\"y_vocab_len\"],\n",
    "                test_dataset[\"y_max_seq_len\"]\n",
    "            )    \n",
    "\n",
    "            out_file.write(decoded_sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    #for i in range(50, 60):\n",
    "\n",
    "    i = 156\n",
    "\n",
    "    input_seq = training_dataset[\"encoder_input_data\"][i]\n",
    "    #input_seq = encode_text_to_input_seq(\"kočka je dírou\")\n",
    "\n",
    "    input_seq = input_seq.reshape((1,training_dataset[\"X_max_seq_len\"]))\n",
    "    decoded_sentence = decode_sequence(\n",
    "        input_seq, encoder_model, decoder_model, \n",
    "        training_dataset[\"y_ix_to_word\"],\n",
    "        training_dataset[\"y_vocab_len\"],\n",
    "        training_dataset[\"y_max_seq_len\"]\n",
    "    )\n",
    "    print('-')\n",
    "    print('Input sentence:', \" \".join(training_dataset[\"X_word_seq\"][i]))\n",
    "    print('Expected sentence:', \" \".join(training_dataset[\"y_word_seq\"][i][1:-1]))\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "\n",
    "\n",
    "    # TODO moses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
