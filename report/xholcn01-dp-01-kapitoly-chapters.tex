\chapter{Úvod}
Přísloví praví \uv{Kolik řečí znáš, tolikrát jsi člověkem}. Schopnost dorozumět se s ostatními lidmi na planetě je nesmírně důležitá a přitom jazyková bariéra je překážkou v mezilidské komunikace už od pradávných let. Proto vznikaly a vznikají jednoduché tištěné a následně digitální slovníky a vědci od počátků vzniku výpočetní techniky zkoumají jak vytvořit funkční překladový systém.

Ideálem je překlad tak jak ho známe ze science fiction materiálů. Dvě osoby, mluvící kompletně jiným jazykem si navzájem rozumí v reálném čase. S rozvojem který nastal v posledních letech, tedy intenzivní rozvoj strojového učení a s nástupem umělé inteligence používající hlubokých neuronových sítí se k tomuto ideálu blížíme mílovými kroky. Automatické rozpoznání mluvené řeči je již ve skvělé kvalitě dostupné v běžných spotřebitelských zařízeních a překlad se taky značně vylepšuje.

Obsahem této práce je návrh a realizace překladového systému schopného naučit se, za pomocí datasetů v různých zdrojových a cílových jazycích, překládat věty mezi těmito jazyky. A to pomocí nejnovějších metod, objevených a široce nasazovaných v posledních letech, používajících rekurentní neuronové sítě s architekturou enkodér-dekodér.

V kapitole \ref{chapter:draft} je neformálně nastíněn návrh a cíl této práce. V následující kapitole \ref{chapter:theory} jsou pak rozebrány důležité pojmy a teorie ze kterých je tato práce vystavěna. Kapitole \ref{chapter:implementation} popisuje co bylo implementováno a kapitola \ref{chapter:results} podává výsledky experimentů.

\chapter{Neformální návrh systému} \label{chapter:draft}
Cílem této práce práce je vytvořit systém pro strojový překlad textu pomocí umělých neuronových sítí. Pro snadnou představu, je to podobné jako to co dělá Google Translator\footnote{translate.google.cz} -- blíže popsáno v článku \cite{googleBridgingGap}. Vezme se věta v původním jazyce a vytvoří se z ní co nejvěrnější překlad v jazyce cílovém a to za pomocí natrénované neuronové sítě. V této kapitole je vysvětleno, jak by takový systém mohl vypadat a co za komponenty potřebuje k tomu aby fungoval.

\begin{description}
  \item[Dataset:] Aby bylo možné nacvičit neuronovou síť pro překlad, je nejprve zapotřebí mít nějaký dataset. Jeden dataset obsahuje texty ve dvou jazycích mezi kterými se má překládat. Tyto texty musí být zarovnané, tak aby si jednotlivé věty v těchto jazycích navzájem odpovídaly. Obecně platí, že čím větší množství použitých dat a čím větší model, tím lepší bude výsledek (článek \cite{googleLimits}). Ukázka datasetu je znázorněna na obrázku \ref{img:dataset}.

  \item[Tokenizer:] Dataset a jeho jednotlivé věty před začátkem trénování sítě je nejprve potřeba připravit. Tokenizer rozdělí věty na jednotlivé tokeny (obrázek \ref{img:tokenization}) a zahodí zvolené nepodstatné vlastnosti, které mohou být třeba velká písmena na počátku vět nebo interpunkce. To usnadňuje práci s datasety a také například snižuje velikost slovníků.

  \item[Slovník:] Slovník se vytvoří jako seznam $n$ nejčastějších slov v datasetu ve vstupním a cílovém jazyce. Čím je slovník menší, tím se zmenší výpočetní požadavky, ale na druhou stranu je potřeba vyřešit při trénování a překladu problém se slovy nevyskytujícími se ve slovníku (popsáno v sekci \ref{subsection:oov}).

  \item[Word Embeddings:] Obecně je možné vytvářet jazykové modely, které generují text po písmenech, částech slov nebo po slovech \cite{mikolovSubwords}. Word Embeddings je další forma předzpracování. Každý token ze vstupního slovníku se převede do vektoru reálných čísel, ve kterém jsou zakódovány některé syntaktické a sémantické vlastnosti daného tokenu, což umožní neuronové síti se učit lépe, než kdyby se použilo například jenom číslo označující pozici tokenu ve slovníku. Více v sekci \ref{subsection:neuralembeddings}.

  \item[Model:] Pro překlad je nejvhodnějším způsobem sequence to sequence (dále seq2seq \cite{seq2seq}) s použitím architektury enkodér-dekodér. Na rozdíl od starších statistických metod překladu, kde se překládalo po frázích, moderní překlad pomocí neuronových sítí probíhá po celých sekvencích (větách). Nejprve enkodér vezme word embedding na vstupu a pomocí rekurentní neuronové sítě (sekce \ref{section:rnn}) převede větu na vstupu do velkého vektoru reprezentující její význam (tzv. myšlenkový vektor -- intuice je taková, že když člověk překládá větu, také nejprve pochopí její význam a poté ji až začne překládat). Dekodér -- taky rekurentní neuronová síť -- následně z tohoto vektoru slovo po slovu vygeneruje výslednou přeloženou větu. Dekodér tedy funguje jako jazykový model (sekce \ref{section:langmodel}), který je na inicializovaný na jednu konkrétní větu.
\end{description}


\begin{figure}[H]
    \begin{center}
            \tmpframe{\includegraphics[width=0.62\linewidth]{img/graphs/draft.pdf}}
    \end{center}
	\caption{Schéma návrhu systému pro překlad. Dataset se předzpracuje pomocí tokenizeru. Do enkodéru vstupují tokeny převedené na embeddings. Enkodér větu zakóduje do velkého "myšlenkového" vektoru ze kterého dekodér generuje překlad.}
	\label{img:draft}
\end{figure}


\chapter{Související teorie a pojmy}\label{chapter:theory}
Účelem této kapitoly je blíže vysvětlit a rozebrat jednotlivé pojmy a komponenty potřebné pro vytvoření překladového systému.



\section{Jazykové modely}\label{section:langmodel}
Zatímco u programovacích jazyků existuje jejich formální definice přesně popisující jejich syntaxy a význam, u přirozených jazyků to tak není. Přirozený jazyk vznikl náhodným způsobem v průběhu staletí a tisíciletí narozdíl od formálně definovaných jazyků, které byly navrženy. Přestože běžný jazyk se řídí nějakými pravidly, existuje značné množství výjimek a odchylek. I napříč tomu si však lidé navzájem rozumí. Problém však je tyto pravidla převést do formálních pravidel, tak aby jim rozuměl počítač. Řešením pro tento problém mohou být jazykové modely, které nevznikají nadefinováním formálních pravidel, ale nacvičením modelu z příkladů. Sekce vychází z práce \cite{nmtThesis} a článku \cite{nmtTutorial}.
\\\\
Jazykový model udává pro každou větu $w$ jaká je její pravděpodobnost. Respektive pro sekvenci slov $w = w_1, w_2..., w_m$ získá pravděpodobnost podle rovnice \ref{figure:probdistr}.

\begin{align}\label{figure:probdistr}
  p(w) = \prod_{i=1}^{m} p(w_i|w_{<i})
\end{align}

Pro každé slovo $w_i$ ze sekvence $w$ určí jaká je jeho podmíněná pravděpodobnost v případě, že se před ním nachází slova $w_{<i}$.

\subsection{N-gram modely}\label{subsection:ngram}
Ve výsledku je pro překladový systém potřeba získat model, který pro zdrojovou větu $F$ vrátí přeloženou větu $E$, tak že $P(E|F)$. N-gram model je však jazykový model, který udává jen pro pravděpodobnost věty P(E) (pro nějaký daný kontext nad kterým se model nacvičil).

Takovýto model umožní zhodnotit přirozenost věty a generovat text podobný tomu, na kterém byl model nacvičen.

\begin{description}
  \item[Zhodnocení přirozenosti:] Pomocí jazykového modelu je možné pro větu $w$ zhodnotit, jak moc je přirozená nebo-li jak moc je pravděpodobné, že by takováto věta mohla existovat v textu na kterém byl model natrénován.
  \item[Generování textu:] Protože model umožňuje pro každé slovo $w_i$ získat pravděpodobnost následujícího slova $w_{i+1}$, je takto možné generovat náhodný, přirozeně (vůči zdrojovému textu) vypadající text. Přesně tato vlastnost je potřeba pro generování překladů.
\end{description}

$N$-gram modely umožňují určit pravděpodobnost následujícího slova ve větě v případě, že se před ním nacházelo $n$ nějakých slov (rovnice \ref{figure:ngram}).

\begin{align}\label{figure:ngram}
    P(x_{i}\mid x_{{i-(n-1)}},\dots ,x_{{i-1}})
\end{align}

Se zvětšujícím se $n$ se výrazně zvětšuje náročnost výpočtu. Tímto způsobem tak není snadné zachytit závislosti mezi slovy vzdálenými od sebe více než pár míst.

\subsection{Log-lineární modely} \label{subsection:loglinear}
Stejně jako v případě $n$-gram modelů (sekce \ref{subsection:ngram}), tyto modely počítají pravděpodobnost následujícího slova $w_i$ při kontextu $w_{<i}$. $N$-gram model počítá pouze s výskytem (identitou) slova. Log-lineární modely pracují s \textbf{rysy} (z anglického features). Rys je něco užitečného ohledně daného slova, co se dá použít pro zapamatování a pro předpověď slova dalšího. Jak už bylo řečeno, u n-gram modelů to je pouze identita minulého slova. Formálněji je rys funkce $\phi(e^{t-1}_{t-n+1})$, která dostane na vstupu aktuální kontext a jako výsledek vrátí reálnou hodnotu -- vektor rysů $x \in \mathbb{R}^N$ popisující kontext při použití $N$ různých rysů.

Stejně jako u $n$-gram modelů, nastává problém když je potřeba zaznamenat vzdálenější závislosti. Například u věty \uv{farmář jí steak} je potřeba zaznamenat pro předpovězení slova \uv{steak} jak jeho předcházející slovo $w_{t_1}=j\acute{\imath}$, tak $w_{t_2}=farm\acute{a}\check{r}$. V případě, že by se použil pouze rys $w_{t_1}$, mohl by model předpovídat i věty, které nedávají smysl. Jako je například \uv{kráva jí steak}. Při použití většího množství rysů vznikají mnohem větší nároky na paměť a výkon a taky na velikost trénovacího datasetu. Řešením těchto problémů může být použití neuronových sítí (sekce \ref{subsection:neuralembeddings}).


\subsection{Neuronové sítě a word embeddings}\label{subsection:neuralembeddings}
Stejně jako předchozí modely i NLM (neural language model) je trénován tak aby předpovídal rozložení pravděpodobností přes slova v cílovém slovníku na základě aktuálního kontextu (rovnice \ref{figure:probdistr}).

Předchozí modely při použití většího datasetu a tím pádem většího slovníku čelí \uv{prokletí} dimenzionality. Jednotlivá slova jsou běžně reprezentována jako \textbf{one-hot vektor} (obrázek \ref{figure:oneHot}). Pro reprezentaci jednoho slova je tak použit rozsáhlý vektor $x_i \in \mathbb{R}^{V}$, kde $V$ je použitý slovník daného jazyka. Většina hodnot, až na hodnotu označující dané slovo, je nulová (řídký vektor nebo-li sparse vector).

\begin{figure}[H]
    \begin{center}
        $V$ = $[$farmář, jí, steak, kráva$]$ \quad
        $
        oneHot_{steak}=
        \begin{bmatrix}
            0 \\
            0 \\
            1 \\
            0 \\
        \end{bmatrix}
        $
    \end{center}
	\caption{One-hot vektor pro slovo \uv{steak} ze slovníku $V$. Slovo je znázorněno jedničkou na třetí pozici, což odpovídá jeho pozici ve slovníku. Všechny ostatní pozice vyplňují pouze nuly (řídký vektor). Pro velký slovník to znamená, že každé slovo zabere značné množství paměti.}
	\label{figure:oneHot}
\end{figure}


NLM se s tímto problémem vypořádává za pomocí takzvaných \textbf{word embeddings}. Word embeddings, jsou na rozdíl od one-hot vektoru vektory reálných čísel (husté nebo-li dense vektory). Ke každému slovu ze slovníku se přiřadí takovýto vektor. Výhodou je, že může nést, narozdíl od pouhé pozice slova ve slovníku, další různé užitečné významy. Třeba pro slovo \uv{kráva}, by ve vektoru mohly být zakódované významy jako podstatné jméno, velký savec atd. Díky tomu může model lépe generalizovat, a slova, která jsou sobě blízká v tomto prostoru, může model brát například jako synonyma.
Nejznámější ukázkou vlastností word embeddings je ukázka \ref{figure:kingQueen} z článku \cite{kingQueen}. \\\\

\begin{figure}[H]
    \begin{center}
        $
          v(kr\acute{a}l) - v(mu\check{z}) + v(\check{z}ena) \approx v(kr\acute{a}lovna)
        $
    \end{center}
	\caption{Ukázka vlastností word embeddings. $\approx$ udává nejbližšího souseda v prostoru. Je vidět, že vektory v sobě nesou určitý sémantický význam. Odečtením hodnoty vektoru slova \uv{muž} se získá jakási podstata slova \uv{král} nebo \uv{kralovat}. Přičtením hodnoty slova \uv{žena} k této dočasné hodnotě se pak získá ženská varianta krále -- královna.}
	\label{figure:kingQueen}
\end{figure}


Existuje několik variant výpočtů word embeddings -- word2vec \cite{word2vec}, glove \cite{glove} a fasttext \cite{fasttext}.


Embbedings jsou vhodné pro \textbf{transfer learning}. To je způsob využití znalostí získaných někde jinde pro jiný problém. Word embeddings je možné buďto získat v průběhu učení modelu nebo použít už předtrénované, připravené pro tento účel. Díky tomu může model získat více znalostí o jednotlivých slovech a celkový výsledek může být výrazně lepší.

\subsection{Zpracování neznámých slov} \label{subsection:oov}
Jazykový model typicky pracuje s pevně danou velikostí slovníku (počtu slov, co se mohou vyskytovat), což je problém, protože překlad je obecně problém s otevřenou slovní zásobou. Existuje-li dataset $\varepsilon_{train}$ obsahující texty na kterých se model bude učit a dataset $\varepsilon_{test}$, který bude sloužit k ověření výkonosti a generalizace modelu, je více než pravděpodobně, že v testovacím setu se budou nacházet slova, která se v trénovacím nenacházela. To znamená, že se v testovacím datasetu budou vyskytovat \textbf{neznámá slova}. Pro pokrytí co největšího množství slov by tak bylo potřeba učit model s co největším slovníkem, to by ale bylo neefektivní a tak naopak může být vhodné omezit celkový počet slov se kterými se bude model trénovat pro zlepšení výkonu a tím pádem ale zvýšit výskyt neznámých slov. Práce \cite{nmtTutorial} uvádí tři běžné způsoby jak se vypořádat s neznámými slovy.

\begin{description}
  \item[Předpokládat že slovník je konečně velký:] V některých případech se dá počítat s tím, že slovník je omezený. Tím pádem se neznámá slova nebo znaky nemohou vyskytovat. Například, když by se trénoval model pouze na znacích ASCII, tak při dostatečně velkém vstupním datasetu by bylo rozumné předpokládat, že se v něm vyskytly všechny znaky a model se je tedy mohl všechny naučit.
  \item[Interpolovat pravděpodobnost pro neznámá slova:] Je možné interpolovat rozložení pravděpodobnosti i přes neznámá slova. Lze natrénovat jazykový model co by po písmenech odhadoval neznámá slova nebo lze odhadnout celkový počet slov ve slovníku a pravděpodobnost $P_{unk}$ pak počítat jako $P_{unk}(e_t) = 1/|V_{all}|$.
  \item[Přidáním speciálního slova <unk>:]\label{description:unk}V případě, že se v trénovacím setu $\varepsilon_{train}$ některá slova vyskytují málo nebo jenom jednou, mohou se nahradit speciálním slovem \emph{<unk>}. S tímto slovem se pak pracuje stejně jako s ostatními. Díky tomu se zredukuje počet slov ve slovníku a tedy náročnost výpočtu. Má však přiřazenou svoji pravděpodobnost a může se tak vyskytnout v předpovědi modelu při generování textu.
\end{description}

Dalším uváděním řešením, je místo slov jako nejmenší jednotky se kterou se pracuje (tokenem), je použití takzvaných \textbf{subword units} nebo-li jednotek menších než slovo \cite{subwords}.

\begin{description}
  \item[Subword units:]\label{subsection:subwords} Kvůli velkému množství slov a jejich tvarů, se obvykle používá velikost slovníků kolem 30 000-50 000 tisíců slov. Kvůli tomu je trénink a překlad náročnější a také to nezaručuje kvalitní překlad pro vzácná slova ani že se nevyskytnou slova neznámá. Značné množství slov je tvořeno několika pod částmi, předložkami a spojeninami (například \uv{kladkostroj}). Ukázalo se, že v případě, že se slova rozdělí na takovéto podčásti tak se:

      \begin{enumerate}
        \item Zmenší velikost slovníku - když by se místo slov pro trénování sítě používaly jednotlivá písmena, velikost slovníku by byla jen samozřejmě nejmenší, ale trénink modelu by byl složitý. Při použití částí slov je možné z nich skládat větší celky a nemusí se tak ve slovníku vyskytovat tolik slov. Je to vhodná kombinace mezi velikostí slovníku a množstvím závislostí co model musí natrénovat.
        \item Zredukuje se výskyt neznámých slov - protože se slovník skládá z jednotek menších jak slovo, je možné z nich seskládat všechna slova z trénovacího datasetu a tím v něm zrušit výskyt neznámých slov. V případě výskytu neznámých slov v době překladu je možné z menších jednotek slovo přeložit po částech.
      \end{enumerate}

      Pro rozložení na menší jednotky se používá \emph{byte pair encoding} (BPE). BPE je jednoduchá kompresní metoda. V práci \cite{subwords} je použita pro spojování písmen nebo sekvencí ve větší celky. Vybere se počet, kolikrát má proběhnout spojování, v každé iteraci se vyberou nejčastěji se vyskytující znaky nebo sekvence ve slovníku a ty se spojí. Nejčastější slova tak zůstanou zachována a ostatní budou rozdělena na různé množství n-gramů. Díky zachování nejčastějších slov, nevzniká výrazný problém s nárůstem délek sekvencí, která by zapříčinila horší učení modelu, protože delší sekvence znamená větší vzdálenost, přes kterou musí model přenést informace.
\end{description}

\section{Rekurentní neuronové sítě}\label{section:rnn}
V této kapitole je popsán základní koncept rekurentních neuronových sítí (RNN\footnote{z anglického recurrent neural network}), jejich srovnání s běžnými neuronovými sítěmi a dále pak popis upravených variant rekurentních sítí -- LSTM (sekce \ref{section:LSTM}) a GRU (sekce \ref{section:GRU}). Sekce vychází z práce \cite{nmtThesis}, práce \cite{nmtTutorial} a článku \cite{understandingLSTM}.


RNN (článek \cite{rnn}) jsou známé již přes dvě desítky let. Úspěšně jsou však používány až v posledních letech. A to hlavně díky vyššímu výpočetnímu výkonu a většímu objemu trénovacích dat, který je v současné době dostupný a také zpracovatelný. Tento druh neuronových sítí je obzvlášť vhodný například pro rozpoznávání psaného písma, rozpoznávání řeči, v kombinaci s konvolučními neuronovými sítěmi pro generování popisků obrázků a co je nejvíce zajímavé pro tuto práci, pro tvorbu jazykových modelů, generátorů textu a tím pádem i pro překlad.

Jejich hlavních výhodou oproti jednoduchým dopředným neuronovým sítím je jejich schopnost držet si vnitřní stav napříč časem. Základní neuronová síť pracuje vždy s aktuální hodnotou $x$ na vstupu, pro kterou pomocí vah $W$ získá výstup $y$ (rovnice \ref{figure:basic-nn}).

\begin{align}\label{figure:basic-nn}
  y = f (x, W)
\end{align}

Pokud pak takováto síť pracuje s nějakou sekvencí měnící se v čase, například se slovy v rámci jedné věty, pro každé slovo na vstupu $x_t$, kde $t$ znázorňuje čas (pozici) slova ve větě, použije stejné váhy pro získání výstupu $y_t$ a nezjistí ani nezachová žádnou úvahu o vzájemném vztahu těchto slov.

RNN tento problém řeší zavedením skrytého stavu $h_t$ a zpětné smyčky (obrázek \ref{img:rnn}). Vstupem dalšího stavu je kromě nového vstupu vždycky také výstup ze stavu minulého. Pro každé $x_t$ ze sekvence se tedy nyní může získat výstup $y_t$ pomocí vnitřního stavu $h_t$ z předchozího kroku $t$ (rovnice \ref{figure:rnn}). Přičemž počáteční stav  $h_0$ je obvykle nastaven na nulu.


\begin{figure}[H]
    \begin{center}
            \tmpframe{\includegraphics[width=1.0\linewidth]{img/RNN-unrolled.png}}
    \end{center}
	\caption{Znázornění RNN -- $x_t$ je vstup, $A$ zastupuje vnitřní chování RNN a $h_t$ je skrytý stav. Rozdíl oproti běžné dopředné neuronové síti je zpětná smyčka a skrytý stav. Pravá část obrázku ukazuje pro lepší představu místo zpětné smyčky rozbalenou strukturu přes jednotlivé časy $t$. Intuitivně se pak dá odhadnout, že RNN umí dobře pracovat s podobnými strukturami jako jsou sekvence a seznamy. Obrázek převzat z \cite{understandingLSTM}.}	
	\label{img:rnn}
\end{figure}

\begin{align}\label{figure:rnn}
  h_t = \begin{cases}
          f(W_{xh}x_t + W_{hh}h_{t-1} + b_h) & \mbox{pokud t $\geq$ 1}, \\
          0 & \mbox{jinak}.
        \end{cases}
\end{align}


$W_{xh}$ znázorňuje váhy pro aktuální vstup, $W_{hh}$ jsou váhy pro skrytý stav z minulého kroku a $b_h$ je bias. Funkce $f$ z rovnice \ref{figure:rnn} je nelineární funkcí a nejčastěji se používá jedna z funkcí \emph{step}, \emph{sigmoida}, \emph{tanh} nebo \emph{relu} (obrázek \ref{img:functions}).

\begin{figure}[H]
    \begin{center}
            \tmpframe{\includegraphics[width=0.45\linewidth]{img/step.pdf}}
            \tmpframe{\includegraphics[width=0.45\linewidth]{img/sigmoid.pdf}}
            \tmpframe{\includegraphics[width=0.45\linewidth]{img/tanh.pdf}}
            \tmpframe{\includegraphics[width=0.45\linewidth]{img/relu.pdf}}
    \end{center}
	\caption{Funkce $step$, $sigmoida$, $tanh$ a $relu$.}
	\label{img:functions}
\end{figure}

Rovnice pro RNN jazykový model jsou následující:

\begin{align}
  m_{t}&=M_{e_{t-1}}\label{figure:lastContext} \\
  h_{t}&=RNN(m_t, h_{t-1}) \label{figure:rnnSimple} \\
  s_{t}&=W_{hs}h_t + b_s \label{figure:rnnSt} \\
  p_{t}&=softmax(s_t) \label{figure:rnnSoftmax}
\end{align}


Kde \ref{figure:lastContext} je aktuální kontext, \ref{figure:rnnSimple} je zjednodušený přepis rovnice RNN \ref{figure:rnn} a rovnice \ref{figure:rnnSoftmax} je funkce softmax (rovnice \ref{figure:softmax}), která vezme všechny hodnoty skóre pro jednotlivá slova a transformuje je do pravděpodobnostního rozložení $p_t$. Díky tomu pak lze již snadno určit, které slovo bude vygenerováno s největší pravděpodobností.


\begin{align}\label{figure:softmax}
  p_t(y)={\frac {e^{p_{t}(y)}}{\sum _{k=1}^{K}e^{p_{t_{k}}}}}
\end{align}
\\


Protože vektor $m$ z rovnice \ref{figure:lastContext} je konkatenací všech předchozích slov (a tedy je to aktuální kontext), model se může naučit kombinaci různých vlastností napříč několika různými slovy z kontextu. V sekci \ref{subsection:loglinear} byl jako problém uveden příklad \uv{Farmář jí maso} a \uv{Kráva jí maso}, kde druhá věta nedává smysl. Při použití RNN by se pro kontext $M_f$  \{farmář, jí\} mohla naučit jedna z jednotek skryté vrstvy $h$ rozpoznat vlastnost "věci které farmář jí" a správně se aktivovat a pak nabízet slova jako \uv{maso} nebo \uv{brambory}. Zatímco pro kontext $M_k$ \{kráva, jí\} by se naučila zase jiná jednotka. RNN je tedy schopná zachytit tyto vzdálenější závislosti. Základní verze RNN je však schopná zachytit závislosti jen do určité vzdálenosti viz \ref{subsection:gradient}.


\subsection{Trénování} \label{subsection:training}
Cílem trénování sítě je nalézt takové parametry $\theta$ (kombinace vah $W$ a biasu $b$) aby byla co nejmenší hodnota takzvané \emph{loss funkce}. Loss funkce vyjadřuje jak moc špatně výstupy sítě odpovídají datům na kterých se síť učí. Průchod sítí a následné vypočítání loss funkce se nazývá dopřednou propagací.


\todo{jakej pouzit vzorec}
\begin{align}\label{figure:loss}
  a
\end{align}

K optimalizaci parametrů pro najití minima loss funkce se používá zpětná propagace. Vypočte se přírůstek pro každý parametr, tak aby síť s novými váhy o něco lépe pracovala a loss funkce se snížila. Existuje více různých metod optimalizace pro tento výpočet, lépe popsaných v práci \cite{gradientDescent}.

Úprava parametrů může probíhat po každém jednom průchodu dat (jedné sekevence) sítí. Takovýto přístup se nazývá \textbf{online} učení. Dalším přístupem je \textbf{učení po dávkách}. V takovém to případě probíhá přepočet parametrů až po průchodu přes $n$ sekvencí. Toto číslo $n$ se nazývá \textbf{batch size}, tedy počet dat v jedné dávce.

\todo{epoch, batchsize....loss, back propagation (over time)}
\todo{z tutorialu, sekce 6.5 strana 35..nejak popsat batch, minibatch, online batch, sentence padding a masking}

\todo{trénování rnn, back propagation through time, have difficulties (\url{http://proceedings.mlr.press/v28/pascanu13.pdf}) learning long term dependencies}
\todo{loss computing}
\todo{gradient computing}

Gradient descent methods
\begin{itemize}
  \item a
\end{itemize}

\url{http://ruder.io/optimizing-gradient-descent/index.html#stochasticgradientdescent}
\todo{jak prelozit gradient descent? popsat co to je, ke cemu to je a pak vypsat ty jednotlivy metody a trochu je popsat}
\url{https://arxiv.org/abs/1609.04747}


\subsection{Mizející a explodující gradient} \label{subsection:gradient}
RNN jsou oproti základním neuronovým sítím schopné zachytit různé závislosti mezi slovy na delší vzdálenosti. I tato schopnost je však velmi limitovaná. Hlavními zdroji problémů jsou \textbf{mizející a explodující gradient} (článek \cite{gradientProblems}).

\todo{patrne v nejake predchozi casti zhruba popsat LOSS, BACKPROPAGATION aby se tady na to pak dalo navazat} v sekci \ref{subsection:training}

Při průběhu učení RNN průběžně vznikají predikce a počítá se \emph{loss} funkce. Následně je potřeba zpětně zpropagovat tuto hodnotu přes všechny (časové) kroky sítě (Back propagation over time -- BPTT). Pokud však není gradient rovný 1, tak se v každém zpětném kroku buďto zmenší a tím pádem se blíží k nule, nebo se naopak zvětší a blíží s k nekonečnu. Ve výsledku je tak gradient buďto příliš malý a nemá tak žádný efekt na úpravu vah nebo jimi pohne příliš a tak zaviní špatné učení se sítě.

Jako možné řešení těchto problémů vznikla varianta rekurentní sítě LSTM (sekce \ref{section:LSTM}).


\subsection{LSTM}\label{section:LSTM}
Long short term memory, dále LSTM, (původní článek \cite{LSTM} a varianta LSTM s forget gate, která se zde používá \cite{forgetLSTM}) nebo-li dlouhá krátkodobá paměť, je varianta RNN navržená jako řešení problémů mizejícího/explodujícího gradientu a vzdálených závislostí.

Stejně jako základní RNN (sekce \ref{section:rnn}), se dá LSTM představit jako opakující se modul v řetězové struktuře (viz obrázek \ref{img:rnn}). Rozdíl je ve vnitřku modulu $A$. Zatímco RNN používá pouze jednu nelineární funkci (rovnice \ref{figure:rnn}), struktura LSTM je složitější (obrázek \ref{img:LSTM} a následující rovnice).

\begin{align}
    u_{t}&=tanh(W_{xu}x_t + W_{hu}h_{t-1} + b_u) \label{figure:update} \\
    f_{t}&=\sigma(W_{xf}x_{t}+W_{hf}h_{t-1}+b_{f}) \label{figure:forgetGate} \\
    i_{t}&=\sigma(W_{xi}x_{t}+W_{hi}h_{t-1}+b_{i}) \label{figure:inputGate} \\
    o_{t}&=\sigma(W_{xo}x_{t}+W_{ho}h_{t-1}+b_{o}) \label{figure:outputGate} \\
    c_{t}&=i_{t}\odot u_{t}+f_{t}\odot c_{t-1}\label{figure:memoryCell} \\
    h_{t}&=o_{t}\odot tanh(c_{t}) \label{figure:hiddenState}
\end{align}

RNN má pouze skrytý stav $h$. LSTM má navíc ještě paměťovou buňku $c$ (rovnice \ref{figure:memoryCell}). Protože gradient této buňky je právě jedna, netrpí tak LSTM problémy ze sekce \ref{subsection:gradient} a mohou tak v ní být zachyceny i vzdálené závislosti.

Rovnice \ref{figure:update} je update funkcí a je ekvivalentem rovnice \ref{figure:rnn} z RNN.
Dále LSTM obsahuje tři různé \textbf{brány}. \textbf{Zapomínací}, \textbf{vstupní} a \textbf{výstupní}. Tyto brány určují a kontrolují co se nachází v paměti $c_t$.

Nejdříve se LSTM rozhodne, jaké informace se vyhodí z paměti. K tomuto slouží již zmíněna zapomínací brána nebo-li forget gate (rovnice \ref{figure:forgetGate}). Například v případě, že síť narazí na vstupu na podstatné jméno, mohla by chtít zapomenout rod posledního podstatného jména, který by si mohla uchovávat pro správné generování sloves v minulém čase.

Dalším krokem je vyhodnocení toho co se má přidat do paměti. Nejdříve vstupní brána nebo-li input gate (rovnice \ref{figure:inputGate}) rozhodne, které hodnoty se změní nebo přidají. V návaznosti na minulý příklad by síť mohla chtít uložit aktuální rod nalezeného podstatného jména. Update funkce (rovnice \ref{figure:update}) vyhodnotí jaké hodnoty se mají přidat.

Následuje aktualizace paměti $c_t$ (rovnice \ref{figure:memoryCell}). V kontextu příkladu by se zahodil rod jak určila zapomínací brána a uložil se nový rod podle vstupní brány.

Posledním krokem je určení toho co vydá LSTM na výstupu (skrytý stav $h_t$). Výstupní brána určí co z paměti $c_t$ má projít (rovnice \ref{figure:outputGate}) a v rovnici \ref{figure:hiddenState} se získá výsledek.

Pravděpodobnosti jazykového modelu se získají rovnicí:
\begin{align}
    p_t = softmax(W_{hs}h_{t} + b_{s})
\end{align}


\begin{figure}[H]
    \begin{center}
            \tmpframe{\includegraphics[width=0.8\linewidth]{img/LSTM.pdf}}
    \end{center}
	\caption{Jeden časový úsek LSTM. $h$ je skrytý stav, $c$ je paměťová buňka a $x$ je vstup. Vnitřní struktura koresponduje s rovnicemi \ref{figure:update} až \ref{figure:hiddenState}. Obrázek převzat z \cite{understandingLSTM}, upraven.}
	\label{img:LSTM}
\end{figure}



\subsection{GRU}\label{section:GRU}
LSTM (sekce \ref{section:LSTM}) je dobrým řešením pro problémy ze sekce \ref{subsection:gradient}. Její struktura je ale dosti komplikovaná a tím pádem i náročná na výpočetní výkon. To podnítilo vznik další varianty RNN -- GRU, nebo-li gated recurrent unit (článek \cite{GRU}), která je o něco jednodušší a proto je možné ji použít pro úsporu výkonu.

\begin{align}
    r_{t}&=\sigma(W_{xr}x_{t}+W_{hr}h_{t-1}+b_{r}) \label{figure:resetGate} \\
    z_{t}&=\sigma(W_{xz}x_{t}+W_{hz}h_{t-1}+b_{z}) \label{figure:updateGate} \\
    \tilde{h}_{t}&=tanh(W_{xh}x_t + W_{hh}(r_t \odot h_{t-1}) + b_h) \label{figure:candidate} \\
    h_{t}&=(1 - z_t)h_{t-1} + z_{t}\tilde{h}_{t}) \label{figure:hiddenStateGru}
\end{align}

GRU má pouze dvě brány a skrytý stav $h$. Nový stav se počítá v rovnici \ref{figure:hiddenStateGru} interpolací mezi minulým stavem $h_{t-1}$ a kandidátem na nový stav $\tilde{h}_{t}$ upravený hodnotou \textbf{update} brány (rovnice \ref{figure:updateGate}). Kandidát se získá v rovnici \ref{figure:candidate}, která je podobná update funkci z RNN (rovnice \ref{figure:rnn}), ale je upravena hodnotou \textbf{resetovací} brány (rovnice \ref{figure:resetGate}). Struktura je vyzobrazená na obrázku \ref{img:GRU}.


\begin{figure}[H]
    \begin{center}
            \tmpframe{\includegraphics[width=0.65\linewidth]{img/GRU.pdf}}
    \end{center}
	\caption{Jeden časový úsek GRU. $h$ je skrytý stav a $x$ je vstup. Vnitřní struktura koresponduje s rovnicemi \ref{figure:resetGate} až \ref{figure:hiddenStateGru}. Obrázek převzat z \cite{understandingLSTM}, upraven.}
	\label{img:GRU}
\end{figure}

\section{Seq2seq model s architekturou enkodér-dekodér} \label{section:encoderdecoder}
V předchozích sekcích se práce zabývá rekurentními neuronovými sítěmi a jazykovými modely na nich postavených. V této sekci bude popsáno jak tyto sítě vzít a poskládat je vhodným způsobem pro překlad vět. Sekce vychází z práce \cite{nmtTutorial}.

\textbf{Seq2seq} (článek \cite{seq2seq}) nebo-li sequence to sequence je způsob překladu po celých větách. Jde o modelování pravděpodobnosti $P(E|F)$ tedy pravděpodobnost výstupu $E$ na základě vstupu $F$ (obrázek \ref{figure:seqProbability}).

\begin{figure}[H]
    \begin{center}
        \setlength{\fboxsep}{8pt}
        \fbox{$W_{in}$ = \uv{Ahoj světe}}
        $\Longrightarrow$
        \fbox{$W_{out}$ = \uv{Hello world}}
        \\ \vspace{5mm}
        $P(W_{out}|W_{in})$
    \end{center}
	\caption{Seq2seq modeluje pravděpodobnost $P(W_{out}|W_{in})$. Znamená to, že se naučí předpovídat větu $W_{out}$ na základě věty $W_{in}$ a tím pádem překládat.}
	\label{figure:seqProbability}
\end{figure}

Pro tento druh překladu celých vět za pomocí rekurentních neuronových sítí se používá model s architekturou \textbf{enkodér-dekodér}. Enkodér i dekodér jsou RNN modely. Enkodér dostane na vstupu větu určenou pro překlad a převede ji (enkóduje) do vektoru reálných čísel -- skrytý stav, takzvaný myšlenkový vektor, vyjadřující význam dané věty. Dekodér inicializovaný tímto stavem generuje (dekóduje z myšlenkového vektoru) přeloženou větu. Díky tomu, že dekodér generuje z významového vektoru, nemusí být vstupní věta stejně dlouhá jako výstupní.


\begin{align}
    m^{(f)}_{t}&=M^{(f)}_{f_t}\label{figure:encoderEmb} \\
    h^{f}_{t}&=\begin{cases}
                    RNN^{(f)}(m^{(f)}_{t},h^{(f)}_{t-1}) & \mbox{pokud t $\geq$ 1},\label{figure:encoderState} \\
                    0 & \mbox{jinak}.
                \end{cases}\\
    m^{(e)}_{t}&=M^{(e)}_{e_{t-1}}\label{figure:decoderEmb} \\
    h^{e}_{t}&=\begin{cases}
                    RNN^{(e)}(m^{(e)}_{t},h^{(e)}_{t-1}) & \mbox{pokud t $\geq$ 1},\\
                    h^{f}_{|F|} & \mbox{jinak}.
                \end{cases}\label{figure:decoderState} \\
    p^{(e)}_{t}&=\mbox{softmax}(W_{hs}h^{(e)}_{t} + b_{s}) \label{figure:resultSoftmax}
\end{align}


Pro každé slovo v čase $t$ ze vstupní sequence $F$ se vyhledá jeho embedding (rovnice \ref{figure:encoderEmb}). Následně se v rovnici \ref{figure:encoderState} spočítá skrytý stav enkodéru. Po projití přes celou vstupní větu by měly uvnitř být uloženy všechny informace potřebné pro inicializaci dekodéru. I pro dekodér se nejprve vyhledá pro vstupní slovo jeho embedding (rovnice \ref{figure:decoderEmb}). Použité slovo není z času $t$, ale z času $t-1$, protože dekodér generuje následující slovo vždy na základě předchozího. V čase $t_0$ se jako vstupní slovo používá \textbf{startovací} symbol <s>. Rovnice pro výpočet skrytého stavu dekodéru (\ref{figure:decoderState}) je prakticky stejná jak u encodéru. Pouze v čase $t_0$ se použije koncový stav enkodéru jako inicializace ze které může dekodér vycházet při překladu -- ve vnitřním stavu je zachycen význam věty, kterou má přeložit. Pravděpodobnostní rozložení se pak jako u všech jazykových modelů spočítá pomocí funkce \emph{softmax} (rovnice \ref{figure:resultSoftmax}).


\subsection{Průběh trénování a generování}

Cílem jazykového modelu je předpovídat následující slovo ve větě. Při trénování se nejprve do enkodéru pošle výchozí věta, aby se získal inicializační stav pro dekodér. Do dekodéru, inicializovaného získaným stavem, se po jednotlivých slovech pošle správně přeložená věta, které předchází \textbf{startovací} symbol. Startovací symbol dekodéru říká, že má začít překládat. Při trénování se mu pak následně posílají korektní další slova, aby se zrychlilo učení. Tato metoda se nazývá \uv{teacher forcing}\label{teacherForcing} (\cite{teacherForcing}). Proces je znázorněný na obrázku \ref{img:seq2seq}. Po naučení modelu, ve fázi generování, se pak do dekodéru posílají slova, která již sám vygeneroval (obrázek \ref{img:inference}). Dekodér generuje tak dlouho, dokud nenarazí na \textbf{koncový} symbol, kterým je v době trénování zakončená každá očekávaná věta. Ve skutečnosti však výstupem dekodéru není přímo slovo, ale rozložení pravděpodobnosti přes všechna slova cílového slovníku získaného funkcí \emph{softmax} v rovnici \ref{figure:resultSoftmax}. Je několik možností jak z tohoto rozložení vybrat konkrétní slovo:

\begin{description}
  \item[Náhodný výběr:] Z rozložení pravděpodobnosti $P(E|F)$ se slovo vybere náhodně.
  \item[Chamtivý výběr:] Chamtivý (greedy) výběr spočívá ve vybrání slova, které získalo největší pravděpodobnost -- argmax($P(E|F)$).
  \item[Paprskové prohledávání:]\label{decsription:beamsearch} Z anglického beam search, paprskové prohledávání najde $n$ výstupů s největší pravděpodobností $P(E|F)$, které drží jako $n$ možných výsledků nebo-li hypotéz. V každém kroku $t$ se každá hypotéza rozšíří o další slovo a ze všech aktuálních hypotéz se zase vybere $n$ nejslibnějších. Až jsou všechny hypotézy ukončeny koncovým symbolem <eos>, vybere se z nich ta s největší pravděpodobností, jako výsledek.
\end{description}


\begin{figure}[H]
    \begin{center}
            \tmpframe{\includegraphics[width=1\linewidth]{img/graphs/seq2seq.pdf}}
    \end{center}
	\caption{Enkodér-dekodér architektura. Enkodér obdrží větu na vstupu a vytvoří inicializační stav pro dekodér (\uv{myšlenkový vektor}). Tímto vektorem je inicializován počáteční stav dekodéru. Ten v době trénování dostává na vstupu správně přeloženou větu (teacher forcing) a době generování na vstup slova co sám vygeneroval. Konečný výstup se získá pomocí vrstvy softmax.}
	\label{img:seq2seq}
\end{figure}

\begin{figure}[H]
    \begin{center}
            \tmpframe{\includegraphics[width=0.8\linewidth]{img/graphs/inference.pdf}}
    \end{center}
	\caption{Ukázka práce dekodéru v době predikce překladu (inference). Na vstup jako první příchází startovací symbol <s>. Následně v každém dalším kroku na vstup dekodéru dostane svůj vlastní výstup z kroku minulého. Takto generuje tak dlouho, dokud nenarazí na koncový symbol </s>.}
	\label{img:inference}
\end{figure}

\subsection{Metody optimalizace} \label{subsection:optimization} V této sekci jsou popsány způsoby jakými lze zlepšit výkon seq2seq modelu.

\begin{description}
  \item[Převrácení vstupu:] \label{desc:revert} Článek \cite{seq2seq} udává, že výrazným způsobem pomůže, když se slova ve vstupní sekvenci převrátí a do enkodéru  se věta předává pozpátku. Pravděpodobně je to díky tomu, že závislosti co by běžně byly vzdálené -- typicky poslední slovo ve vstupní větě a jeho přeložená varianta v přeložené větě -- jsou si takhle blíž. Díky tomu se může model snáz a rychleji učit.
  \item[Obousměrný enkodér:] \label{desc:biriectional} Zatímco převrácení vstupu pomůže jen pokud jsou slova ve větách překládaných jazyků na podobných místech (což není pravda napříč všemi jazyky), tato varianta je spolehlivější. Místo jednoho enkodéru se použijí dva a každý z nich projde větu jedním směrem. Jejich výsledky se pak spojí do jednoho skrytého stavu $h$, kterým se již běžně inicializuje dekodér.
  \item[Hloubka sítí:] Enkodér i dekodér jsou RNN a mohou obsahovat více skrytých vrstev (ať již základní varianty, LSTM nebo GRU). Článek \cite{googleBridgingGap} udává, že více vrstev může do určité hloubky pomoci. V práci jich použili 8 jak pro enkodér tak dekodér. Při použití většího množství již má model problém se úspěšně učit.
  \item[Dropout:] \label{desc:dropout} Při trénování neuronových sítí může dojít k přetrénování -- síť se naučí podávat správné výsledky pro trénovací data, ale bude mít malou nebo žádnou schopnost generalizce, tedy nebude fungovat nad jinými než trénovacími daty. Dropout (\cite{dropout}), je regulační metoda používaná k předejití přetrénovaní. Metoda spočívá v náhodném zahazování výsledků některých neuronů v době trénovaní, čímž se snaží síť udělat více robustní, protože se síť nemůže spoléhat na výstupy konkrétních neuronů.
  \item[Reziduální propojení:] \label{residuals} V práci \cite{googleBridgingGap} doporučují při použití více vrstev LSTM použít takzvané reziduální propojení nebo zapojení mezi vrstvy. Podle jejich experimentů při použití většího počtu vrstev a běžném zapojení, začne být učení pomalé a složité, pravděpodobně kvůli explodujícímu a mizejícímu gradientu (\ref{subsection:gradient}). Řešením je použití reziduálního propojení vrstev. Běžně by do vrstvy $LSTM_0$ vstupoval vstup $x_0$ a do vrstvy $LSTM_1$ vstup $x_1$, který je výstupem vrstvy $LSTM_0$. Při reziduálním propojení vstupuje da každé následující vrstvy výstup z vrstvy minulé sečtený dohromady se vstupem minulé vrstvy. Tedy do $LSTM_1$ vchází $x_1+x_0$ (obrázek \ref{img:residuals}).

\end{description}

\begin{figure}[H]
    \begin{center}
            \tmpframe{\includegraphics[width=0.9\linewidth]{img/graphs/residuals.pdf}}
    \end{center}
	\caption{Rozdíl mezi normálním a reziduálním propojení pro zlepšení učení více vrstvých LSTM. Levý obrázek znázorňuje běžné zapojení zatímco na pravém je ukázka reziduálního propojení. Do každé (kromě první) vrstvy $LSTM_i$ vstupuje součet výstupu vrstvy minulé $x_i$ sečtený se vstupem minulé vrstvy $x_{i-1}$.}
	\label{img:residuals}
\end{figure}


\subsection{Překlad s jedním modelem mezi více jazyky}
Doposud udávané informace se týkaly překladu z jednoho jazyka do druhého, tedy existují páry vět mezi dvěmi jazyky, enkodér se natrénuje na větách ve zdrojovém jazyce a dekodér se naučí překládat pomocí vět v cílovém jazyce.

Google ve své práci \cite{googleMultiLingual} ukazuje, že lze se stejným enkodér-dekodér modelem, bez jeho úprav, lze překládat mezi několika jazyky.
Jediným rozdílem je přidání speciálního tokenu před zdrojové věty, který udává, do jakého jazyka se má daná věta překládat, znázorněno na obrázku \ref{figure:multiLingDataset}. V práci se používá slovník wordpieces (\ref{subsection:subwords}) natrénovaný a sdílený přes všechny použité jazyky.


\begin{figure}[H]
    \begin{center}
        \begin{tabular}{|c|}
          \hline
          Ahoj světe. \\
          Venku prší. \\
          \vdots \\
          Farmář jí steak. \\
          \hline
        \end{tabular}
        $\Longleftrightarrow$
        \begin{tabular}{|c|}
          \hline
          Hello world. \\
          It's raining outside. \\
          \vdots \\
          Farmer eats steak. \\
          \hline
        \end{tabular}
    \end{center}
    \begin{minipage}[t]{1\linewidth}
        \centering
        \subcaption{Příklad párů vět mezi dvěmi jazyky.}\label{subcap:singlePair}
    \end{minipage}%

    \begin{center}
        \begin{tabular}{|c|}
          \hline
          <en> Ahoj světe. \\
          <cs> Hello world. \\
          \vdots \\
          <fr> Hola Mundo. \\
          \hline
        \end{tabular}
        $\Longleftrightarrow$
        \begin{tabular}{|c|}
          \hline
          Venku prší. \\
          It's raining outside. \\
          \vdots \\
          Bonjour monde \\
          \hline
        \end{tabular}
    \end{center}
    \begin{minipage}[t]{1\linewidth}
        \centering
        \subcaption{Příklad párů vět se speciálním tokenem, který odlišuje do jakého jazyka má být věta přeložena. Model může překládat z vícero do vícero jazyků.}\label{subcap:multiLingual}
    \end{minipage}%

	\caption{Obrázek ukazuje rozdíl mezi páry vět použitými při běžném překladu mezi dvěmi jazyky (\ref{subcap:singlePair}) a při použití stejného modelu pro překlad mezi více jazyky (\ref{subcap:multiLingual}). Je potřeba přidat počáteční token, který pro enkodér označuje jazyk, do kterého se věta má přeložit.}
	\label{figure:multiLingDataset}
\end{figure}

Model může překládat v různých kombinacích, seřazeno od nejjednodušší po nejsložitější:
\begin{description}
  \item[N:1] z více jazyků do jednoho
  \item[1:N] z jednoho jazyka do vícero
  \item[M:N] z více jazyků do vícero jazyků
\end{description}

Zajímavým zjištěním je, že model je schopný naučit se generovat překlady mezi kombinacemi jazyků, pro které nebyl explicitně natrénován, takzvané \emph{zero-shot} překlady. Pokud se model tranzitivně natrénuje například na překlad jazykových kombinací Čeština$\Rightarrow$Angličtina a Angličtina$\Rightarrow$Francouzština, je následně schopný generovat relativně rozumné překlady mezi párem Čeština$\Rightarrow$Francouzština.


\section{Vyhodnocování vlastností překladových systémů}
\todo{\url{https://en.wikipedia.org/wiki/Evaluation_of_machine_translation} vsechny nebo jenom BLEU? muze to byt kdyztak nekde dole, pokud jenom BLEU?}

\subsection{BLEU} \label{subsection:bleu}
\emph{BLEU} \cite{BLEU} nebo-li bilingual evaluation understudy, je algoritmus pro automatické hodnocení kvality překladu získané strojovým překladem vůči původnímu zdroji. BLEU je jedním z nejpopulárnějších způsobů hodnocení kvality systému mezi výzkumníky. Snaží se hodnotit takovým způsobem, aby čím větší skóre znamenalo, tím blíže se překlad blíží k něčemu co by přeložil profesionální lidský překladatel.

BLEU je počítáno pro jednotlivé věty, které porovnává s jedním nebo více referenčními překlady. Skóre se pak zprůměruje přes celý dataset. Algoritmus porovnává shody v n-gramech a výsledkem je skóre 0--1 respektive 0--100\%.



\section{Attention? nebo ne kdyz ji nepouziju}

\chapter{Implementace} \label{chapter:implementation}
Tato kapitola popisuje všechny autorem vytvořené a použité části. Sekce \ref{section:datasets} je o výběru a předzpracování datasetů. Následující sekce \ref{section:baseline} se zabývá vytvořením baseline systému, vůči kterému se porovnávají výsledky v kapitole \ref{chapter:results}. Poslední sekce této kapitoly (\ref{section:nmtSystem}) popisuje vytvořený překladový systém.

\section{Datasety}\label{section:datasets}
Jako dataset (nebo korpus) se v této práci považují dva soubory. Každý ze souborů obsahuje věty v jednom jazyce. Na každém řádku souboru je jedna věta a ta svým významem odpovídá větě na stejném řádku v jazyce druhém. Dataset nese nějaký název (název souboru stejný pro oba jazykové soubory) a jako koncovku používá dvou písmenou zkratku jazyka. Pro lepší představu je přiložen obrázek \ref{img:dataset}.

\begin{figure}[H]
    \begin{center}
        \begin{tabular}{|c|}
          \hline
          exampleDataset.cs\\
          \hline
          Ahoj světe. \\
          Venku prší. \\
          \vdots \\
          Farmář jí steak. \\
          \hline
        \end{tabular}
        $\Longleftrightarrow$
        \begin{tabular}{|c|}
          \hline
          exampleDataset.en\\
          \hline
          Hello world. \\
          It's raining outside. \\
          \vdots \\
          Farmer eats steak. \\
          \hline
        \end{tabular}
    \end{center}
	\caption{Ukázka datasetu. Dataset se jmenuje \uv{exampleDataset} a je rozdělen na český seznam vět (koncovka \uv{cs}) a anglický seznam vět (koncovka \uv{en}). Na každém řádku seznamu vět jednoho jazyka je jedna věta odpovídající si s větou na stejném řádku v jazyce druhém.}
	\label{img:dataset}
\end{figure}

\subsection{Předzpracování} \label{subsection:preparing}
Před použitím na trénování a vyhodnocování překládacího systému je potřeba datasety vhodným způsobem předpřipravit. Tato sekce popisuje aplikované metody. Všechny použité skripty jsou dostupné na githubu programu Moses \footnote{\url{https://github.com/moses-smt/mosesdecoder}}. Cílem je snížit velikost výsledných slovníků a zbavit se nevhodných vět.

\begin{description}
  \item[Tokenizace:] Věty se rozdělí na jednotlivé tokeny oddělené mezerou. V případě běžných slov to znamená, že se nic nezmění. Oddělí se však například interpunkce. K tokenizaci se používá skript z nástroje Moses \emph{tokenizer.perl}. Každý jeden token je ve výsledku jedno slovo ze slovníku a musí tak pro něj existovat jeho embedding nebo se převede na <unk> symbol.
\end{description}

\begin{figure}[H]
    \begin{center}
     \setlength{\fboxsep}{8pt}
        \fbox{Myslím,\textvisiblespace že\textvisiblespace venku\textvisiblespace prší!}
        $\Longrightarrow$
        \setlength{\fboxsep}{8pt}
        \fbox{Myslím\textvisiblespace ,\textvisiblespace že\textvisiblespace venku\textvisiblespace prší\textvisiblespace  !}
    \end{center}
	\caption{Ukázka tokenizace. Věty se rozdělí po jednotlivých tokenech a každý z nich je oddělen mezerou. Pro lepší znázornění je v ukázce jako mezera použit znak \uv{\textvisiblespace}.}
	\label{img:tokenization}
\end{figure}


\begin{description}
  \item[Truecasing:] Velká písmena na začátku vět se převedou na malá nebo se zachovají podle toho v jaké formě se slovo nejčastěji vyskytuje v celém datasetu. Velká písmena tak zůstanou jen tam kde je to běžná podoba slova (například u jmen). Díky tomu se sníží počet slov ve slovníku. Pro truecasing se používají skripty z nástroje Moses \emph{train-truecaser.perl} a \emph{truecase.perl}.
\end{description}

\begin{description}
  \item[Vyčištění:] Zahodí se prázdné či špatně zarovnané řádky. Dále se zkrátí věty na maximální délku 15 tokenů. Příliš dlouhé sekvence by znamenaly značnou zátěž na paměť a rychlost trénování překladového systému. Pro vyčištění je použit skript z nástroje Moses \emph{clean-corpus-n.perl}.
\end{description}

\begin{description}
  \item[Rozdělení slov na menší jednotky:]\label{description:bpeApplication} Jak je popsáno v sekci \ref{subsection:subwords}, může být pro trénování výhodné nepoužívat jako nejmenší jednotku slova, ale jejich části. Pro aplikování BPE je použita knihovna \emph{subword-nmt}\footnote{\url{https://github.com/rsennrich/subword-nmt}}. Podle doporučení se BPE vytváří dohromady nad datasety zdrojového i cílového jazyka. Zvolený počet \emph{merge} operací je 15000.
\end{description}


\section{Baseline systém v Moses}\label{section:baseline}
Moses \cite{moses} je nástroj na vytváření statistických strojových překladových systémů. Vzniklý model bude sloužit jako baseline vůči kterému se porovnají výsledky implementovaného překladového systému (sekce \ref{section:nmtSystem}). Kromě toho se také používají některé skripty z tohoto nástroje pro přípravu datasetů (sekce \ref{subsection:preparing}) a získání skóre BLEU. Konkrétní postup jeho přípravy je dostupný na stránkách Moses \footnote{\url{statmt.org/moses/?n=Moses.Baseline}}, byly použity výchozí nastavení (viz soubor \emph{run\_moses.sh}). \todo{muzu odkazovat na prilozene soubory?}

\section{Překladový systém}\label{section:nmtSystem}
Pro implementaci překladového systému byl zvolen jazyk Python\footnote{\url{python.org}} v jeho verzi 3.6. Na výběr bylo z několika vhodných knihoven/frameworků pro práci se strojovým učením:

\begin{itemize}
  \item Tensorflow -- je open source knihovna, která původně vznikla v rámci výzkumného týmu Google Brain uvnitř společnosti Google. Tensorflow používá pro výpočty graf, kde jednotlivé uzly reprezentují operace a hrany reprezentují datové struktury (tensory). Tensorflow se stala velice populární v oblasti vývoje neuronových sítí.
  \item Theano -- knihovna pro efektivní práci s mnoho rozměrnými poli. Využívá pole z hojně používané pythoní knihovny Numpy. Nedávno se knihovna dostala na verzi 1.0 a naráz s tím se ukončil její vývoj.
  \item CNTK -- Cognitive Toolik je open-source nástroj deep learning od firmy Microsoft. Poskytuje API pro jazyky C\#, C++ i Python. Pro výpočty také používá graf, kde listy reprezentují vstupní hodnoty nebo parametry a ostatní uzly reprezentují maticové operace.
  \item Keras -- je pythoní knihovna poskytující vysoko úrovňové API pro deep learning. Je vysoce modulární a určená pro snadné prototypování. Knihovna běží nad backendem, který používá pro výpočty. Backend může být jedna z předchozích knihoven -- Tensorflow, Theano nebo CNTK.
\end{itemize}


\subsection{Balíček nmt}
Překladový systém je naimplementován formou pythoního balíčku (knihovny), který je zveřejněn na githubu \footnote{https://github.com/jojkos/neural-machine-translation}. Pro jeho implementaci byla zvolena knihovna Keras \cite{keras} pro svůj jednoduchý a více intuitivní přístup a také pro množství návodů, které pro tuto knihovnu vznikají. Jako backend pro Keras je použit framework Tensorflow \cite{tensorflow}.
\\\\
Slovníky výchozího a cílového jazyku se omezují na zvolenou maximální velikost a neznámá slova jsou nahrazeny symbolem <unk> (viz sekce \ref{subsection:oov}). Knihovna umožňuje použití před trénovaných word embeddings (\ref{subsection:neuralembeddings}) ve formátu \emph{fastText}. Jsou implementovány a použity optimalizace popsané v sekci \ref{subsection:optimization} -- používá se obousměrný enkodér, je možné vytvořit model s různou hloubkou sítí a při trénování se používá dropout a teacher forcing (\ref{teacherForcing}). Vrstvy enkodéru a dekodéru jsou tvořeny jednotkami LSTM (\ref{section:LSTM}) s použitím reziduálních propojení. Pro co nejpřesnější generování předpovídaných vět je použito paprskové prohledávání (\ref{decsription:beamsearch}).

Pro potřeby trénování a překladu jsou pomocí knihovny Keras vystavěny tři modely, které spolu sdílejí vrstvy a jejich natrénované váhy -- úplný model enkodér-dekodér použitý při trénovaní (obrázek \ref{img:model}) (do modelu vstupují věty ve výchozím jazyce, věty v cílovém jazyce pro použití teacher forcing), model enkodéru použitý při překladu pro získání inicializačních hodnot pro dekodér (obrázek \ref{img:encodermodel}) a model dekodéru použitý při překladu pro generování vět (obrázek \ref{img:decodermodel}).


\begin{figure}[H]
    \begin{center}
            \tmpframe{\includegraphics[width=0.95\linewidth]{img/model.pdf}}
    \end{center}
	\caption{Vrstvy a jejich propojení v enkodér-dekodér modelu použitém při trénovaní za použití 1 vrstvy LSTM pro enkodér a 1 vrstvy LSTM pro dekodér. Do \emph{encoder\_input} přicházejí sekvence ve výchozím jazyce. V \emph{input\_embeddings} se převedou do podoby embeddings vektoru. Následuje obousměrná vrstva enkodéru. Do \emph{decoder\_input} přichází sekvence začínající startovacím tokenem <s> následované korektním překladem v cílovém jazyce, protože se používá teacher forcing. V \emph{target\_embeddings} se převedou do podoby embeddings vektoru. Následuje vrstva dekodéru, který výsledným stavem enkodéru -- zprůměrovaným, protože enkodér je obousměrným, takže má dvojnásobně veliký vnitřní stav. Poslední vrstva \emph{output\_layer} s aktivační funkcí \emph{softmax} vrací pravděpodobnosti pro všechna slova z cílového slovníku.}
	\label{img:model}
\end{figure}

\begin{figure}[H]
    \begin{center}
            \tmpframe{\includegraphics[width=0.6\linewidth]{img/encoder_model.pdf}}
    \end{center}
	\caption{Vrstvy a jejich propojení v modelu enkodéru použitém při generování překladů při použití 3 vrstev LSTM. 1. vrstva je obousměrná, další vrstvy jsou již jen dopředné. Protože velikost embeddings se nemusí shodovat s počtem použitých jednotek v LSTM a první vrstva je obousměrná, reziduální propojení se používá až od třetí vrstvy a dál.}
	\label{img:encodermodel}
\end{figure}

\begin{figure}[H]
    \begin{center}
            \tmpframe{\includegraphics[width=1.0\linewidth]{img/decoder_model.pdf}}
    \end{center}
	\caption{Vrstvy a jejich propojení v modelu dekodéru použitém při generování překladů při použití 3 vrstev LSTM. Každá vrstva dekodéru je inicializována koncovým stavem enkodéru. Protože velikost embeddings se nemusí shodovat s počtem použitých jednotek v LSTM, reziduální propojení se používá až od druhé vrstvy a dál.}
	\label{img:decodermodel}
\end{figure}


Metodou \emph{fit} se spustí trénování modelu \todo{nekde vysvetlit pojmy jako epocha, batch size a vsechny tyhle dalsi veci..kde a jak?}


\subsubsection{Stručný popis obsahu repozitáře \texttt{nmt}}
\begin{description}
  \item[třída Translator:] Je hlavní třídou. Přijímá veškerá nastavení týkající se modelu, trénovací a testovací dataset, vytváří model a provádí trénink, překlad a jeho vyhodnocení.

      Přehled hlavních metod:
      \begin{itemize}
                  \item metoda \texttt{fit} zahajuje trénování
                  \item metoda \texttt{translate\_test\_data} přeloží testovací dataset s pomocí natrénovaného modelu
                  \item metoda \texttt{get\_bleu\_for\_test\_data\_translation} vyhodnotí skóre BLEU pro vzniklý překlad
      \end{itemize}
  \item[třída Dataset:] Drží v sobě dataset v jeho tokenizované formě, tedy pole sekvencí jednotlivých vět. Řeší jeho načtení ze souboru a zpracováno do podoby vhodné pro trénování.
  \item[třída Vocabulary:] Na základě všech sekvencí v datasetu daného jazyka vezme a drží v sobě \emph{x} nejčastějších slov, se kterými pak model pracuje.
  \item[třída Candidate:] Je pomocná třída použitá při výpočtu paprskového prohledávání \ref{decsription:beamsearch}. Drží v sobě hodnotu jednoho z aktuálně rozgerovaných kandidátních překladů.
  \item[třída SpecialSymbols:] Obsahuje výčet speciálních symbolů použitých při trénování. \begin{itemize}
                  \item \emph{\_PAD} pro zarovnávací nulu
                  \item \emph{\_GO} pro startovací token
                  \item \emph{\_EOS} pro koncový token
                  \item \emph{\_UNK} pro neznámý token
                \end{itemize}
  \item[třída Utils:] Obsahuje pomocné metody pro úpravu dat, výpočty a pro volání přiložených skriptů jako je \emph{SubwordNMT} a výpočet BLEU skóre.
  \item[knhovna SubwordNMT:] Součástí repozitáře je knihovna \emph{SubwordNMT} použitá pro aplikování BPE (\ref{description:bpeApplication}). Knihovna je přidaná jako \emph{git submodule}.
  \item[testy:] Repozitář \emph{nmt} obsahuje sadu testů pokrývajících jeho funkcionalitu, převážně jednotlivých metod (unit testy) a celkového fungování trénování a překladu. Testy jsou implementovány ve frameworku \emph{pytest}.
\end{description}

\subsection{Rozdělení dat podle velikosti}\label{subsection:Bucketing}
Všechny sekvence při trénování v rámci jedné dávky musí mít stejnou délku. Věty však mají délku různou. To bývá řešeno tak, že se každá věta zarovná pomocí nul na délku nejdelší věty (obrázek \ref{figure:padding}). Na první pohled může být jasné, že pokud se všechny věty v datasetu zarovnají na velikost nejdelší věty, způsobí to zbytečně větší vytížení paměti a delší trénování modelu.

\begin{figure}[H]
    \begin{center}
        \begin{tabular}{|l|}
          \hline
          \uv{Venku dneska svítí slunce .} \\
          \uv{Ahoj světe .} \\
          \hline
        \end{tabular}
        $\Longrightarrow$
        \begin{tabular}{|l|}
          \hline
          {[1, 2, 3, 4, 5]} \\
          {[6, 7, 5, 0, 0]} \\
          \hline
        \end{tabular}
    \end{center}
	\caption{Příklad zarovnání sekvencí na stejnou délku. V levé tabulce jsou ukázány věty po tokenizaci, první věta má pět tokenů a druhá tři. V druhé tabulce jsou pak věty převedené do matic, kde číslo vyjadřuje pozici daného slova ve slovníku respektive v embeddings a 0 je zarovnání.}
	\label{figure:padding}
\end{figure}

Tento problém je omezen pomocí rozdělení vět v datasetech do skupin podle jejich velikostí. Věty o podobné velikosti jsou shluknuty do jedné skupiny a tím se zamezí zbytečnému nárůstu požadavků na paměť. Dávky se pak při generování (\ref{subsection:generation}) berou vždy z jedné skupiny, tak aby všechny sekevence v jedné dávce byly stejně dlouhé.

\subsection{Generování dávek}\label{subsection:generation}
Protože se data při trénování upravují datasety do podoby velkých matic, není možné, kvůli paměťovým omezením, vyrobit jednu matici pro všechny data zaráz. Proto se dávky (batche) v průběhu trénování vytváří postupně, pomocí generátorové funkce. Ta před každou epochou (jedna epocha znamená, že modelem prošly všechny vstupní data), zamíchá vstupní data -- správné míchání vstupních dat je důležité pro dobrou konvergenci modelu -- a postupně generuje matice s jednotlivými dávkami. V případě, že je zapnuté dělení do skupin podle velikosti, probíhá míchání na úrovní skupin a dávek tak, aby se model nepřetrénoval v jednu chvíli na například krátké věty a v jiné části epochy zase na dlouhé. Je použit \texttt{random.seed(0)} pro opakovatelnost experimentů.

\chapter{Experimenty a výsledky} \label{chapter:results}
V této sekci jsou prezentovány datasety které byly použity pro experimenty provedené s vytvořeným překladovým systémem a nástrojem Moses.

V experimentech \ref{subsection:experimentsOptimal} byl systém natrénován pro překlad z češtiny (cs) do angličtiny (en). Tyto experimenty byly provedeny za účelem najití nejlepších hyperparametrů pro model a pro jeho otestování.

V experimentech \todo{xx---yy} byl systém natrénován pro překlad mezi s parametry, které se určili podle předchozích experimentů.

Pro všechny experimenty byly použity předučené word embeddings, jak pro vstupní tak výstupní jazyk, poskytnuté firmou Facebook\footnote{\url{https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md}}.

Každý experiment běžel tak dlouho, dokud se 5 epoch nezlepšil výsledek loss funkce (\ref{subsection:training}, křížová entropie) počítané pro validační dataset. Z experimentu se ukládají váhy modelu s nejlepším výsledkem loss funkce, jinak se zahazuje.

Velikost jedné trénovací dávky je 128. Je použito generování dávek (\ref{subsection:generation}) a rozdělení dat podle velikosti (\ref{subsection:Bucketing}).

Hodnotící metrikou je standardní skore BLEU (\ref{subsection:bleu}). Pro jeho výpočet byl použit skript \emph{multi-blue.pl} dodávaný s nástrojem Moses.


\section{Použité datasety}
Všechny použité datasety pochází z každoročně publikovaného překládacího úkolu konference WMT. Konkrétně jsou použity datasety z WMT17\footnote{\url{http://data.statmt.org/wmt17/translation-task/preprocessed/}} obsahující textová data z různých domén, jako jsou titulky, knihy, noviny a internet. Jsou dostupné různé jazykové kombinace, zvolený použitý překlad je z češtiny do angličtiny.

Stažený dataset byl rozdělený na několik menších datasetů, které spolu nesdílí páry vět:
\begin{description}
  \item[Trénovací malý:] Obsahuje 1000000 párů vět náhodně vybraných ze staženého datasetu. Je určený pro trénování v experimentech, pomocí kterých se hledají optimální hyperparametry modelu a pro trénování baseline systému.
  \item[Trénovací velký:] Obsahuje 33962614 párů vět náhodně vybraných ze staženého datasetu. Je určený pro finální otestování modelu s nejlepšími výsledky nad malým trénovacím datasetem.
  \item[Validační:] Obsahuje 2000 párů vět náhodně vybraných ze staženého datasetu. Je určený pro testování modelu trénovaném s malým trenovacím datasetem, za účelem hledání optimalních hyperparametrů modelu.
  \item[Testovací:] Obsahuje 2000 párů vět náhodně vybraných ze staženého datasetu. Je určený pro finální otestování modelu natrénovaného s velkým trenovacím datasetem. Používá se místo validačního modelu aby se vyzkoušelo, že výsledný model je skutečně schopný generalizace a nebyl jen úzce přizpůsoben pro dobré výsledky s validačním datasetem.
\end{description}

V případě, že byl dataset v experimentu předzpracován pomocí subword-nmt pro rozdělení slov na menší jednotky, byl vždy přeložený text před vyhodnocením BLEU skóre převeden zpátky na celá slova.

\section{Baseline systém v Moses}
Systém (popsaný v \ref{section:baseline}) byl natrénován za účelem porovnání jeho výsledku s výsledky prezentovaného systému. Pro trénování byl použit \textbf{trénovací malý dataset} a pro vyhodnocení \textbf{testovací dataset}. Výsledek je v tabulce \ref{table:baseline}.

\begin{figure}[H]
    \begin{center}
        \begin{tabular}{c|c}
          & baseline systém v Moses  \\
          \hline
          BLEU skóre & 23.08\\
          \hline
        \end{tabular}
    \end{center}
	\caption{Výsledky modelu natrénovaného pomocí statistického nástroje Moses.}
	\label{table:baseline}
\end{figure}

\section{Experimenty}

\subsection{Hledání optimálních hyperparametrů}\label{subsection:experimentsOptimal}
V této sekci je popsán průběh a výsledky experimentů prováděných za účelem nalezení co nejlepších hyperparametrů modelu. Všechny experimenty jsou provedeny za pomocí malého trénovacího datasetu a překlad je testován pomocí validačního datasetu.

\subsubsection{1. experiment}
Parametry:
\begin{itemize}
  \item Používají se celá slova
  \item 1 vrstva LSTM v enkodéru o velikosti 512
  \item 1 vrstva LSTM v dekodéru o velikosti 512
  \item velikost slovníku je omezena na 15000
  \item paprskové prohledávání o velikosti 1
\end{itemize}

Trénování bylo ukončeno po 9. epoše. Nejlepší dosažený výsledek \textbf{loss funkce} pro validační dataset je \textbf{2.8550}. \textbf{Skóre BLEU} překladu vygenerovaného pro validační dataset je \textbf{7.26}.




\begin{figure}[H]
    \begin{center}
            \begin{tikzpicture}
            \begin{axis}[grid=both,
                         width=0.5\linewidth,
                         xlabel=epocha,
                         ylabel=loss,
                         xtick=data]
                \addplot table [x=Step, y=Value, col sep=comma] {img/experiments/run_G_final1_logs-tag-loss.csv};
            \end{axis}
            \end{tikzpicture}   
            \begin{tikzpicture}
            \begin{axis}[grid=both,
                         width=0.5\linewidth,
                         xlabel=epocha,
                         ylabel=loss,
                         xtick=data]
                \addplot table [x=Step, y=Value, col sep=comma] {img/experiments/run_G_final1_logs-tag-val_loss.csv};
            \end{axis}
            \end{tikzpicture}
    \end{center}
	\caption{Průběh vývoje hodnot loss funkce po jednotlivých epochách. Levý graf je pro trénovací dataset a pravý pro validační dataset.}
\end{figure}



\subsubsection{2. experiment}

\begin{enumerate}
  \item nejakej zakladni bez BPE (final1)
  \item stejnej bez BPE, ale s vyrazne vetsim slovnikem
  \item stejnej, ale s BPE a slovnikem jak ten prvni (oduvodnit pocet bpe operaci, ze 15k je snesitelna velikost slovniku)
  \item BPE bude nejlepsi? kdyz jo, tak s vic jednotkama
  \item s tim poslednim (protoze je nejlepsi) vyzkouset dropout 0.1, 0.2, 0.3
  \item 2 vrstvy pro encoder (s nejlepsim dropautem, takze 0.1)
  \item 2 vrstvy pro decoder
  \item 2 vrstvy pro oboje
  \item 4 vrstvy pro oboje (finalni model?)
\end{enumerate}


\begin{figure}[H]
    \begin{center}
        \begin{tabular}{c|c|c}
          & baseline systém & prezentovaný systém \\
          \hline
          BLEU skóre & 14.0 & 2.03\\
          \hline
        \end{tabular}
    \end{center}
	\caption{Porovnání výsledků baseline systému vytvořeném v nástroji Moses a překladového systému, který je výsledkem této práce.}
	\label{table:results}
\end{figure}

\subsubsection{3. experiment}

\subsubsection{4. experiment}

\subsubsection{5. experiment}

\begin{figure}[H]
    \begin{center}
            \begin{tikzpicture}
            \begin{axis}[grid=both,
                         width=0.48\linewidth,
                         xlabel=epocha,
                         ylabel=loss,
                         xtick={0,2, ..., 14},
                         legend entries = {dropout=0.1, dropout=0.2, dropout=0.3},]]
                \addplot table [x=Step, y=Value, col sep=comma] {img/experiments/run_G_final5-2_logs-tag-loss.csv};
                \addplot table [x=Step, y=Value, col sep=comma] {img/experiments/run_G_final5_logs-tag-loss.csv};
                \addplot table [x=Step, y=Value, col sep=comma] {img/experiments/run_G_final5-3_logs-tag-loss.csv};
            \end{axis}
            \end{tikzpicture}
            \begin{tikzpicture}
            \begin{axis}[grid=both,
                         width=0.48\linewidth,
                         xlabel=epocha,
                         ylabel=loss,
                         xtick={0,2, ..., 14},
                         legend entries = {dropout=0.1, dropout=0.2, dropout=0.3},]]
                \addplot table [x=Step, y=Value, col sep=comma] {img/experiments/run_G_final5-2_logs-tag-val_loss.csv};
                \addplot table [x=Step, y=Value, col sep=comma] {img/experiments/run_G_final5_logs-tag-val_loss.csv};
                \addplot table [x=Step, y=Value, col sep=comma] {img/experiments/run_G_final5-3_logs-tag-val_loss.csv};
            \end{axis}
            \end{tikzpicture}
    \end{center}
    \caption{asd}
\end{figure}

\subsubsection{6. experiment}

\subsubsection{7. experiment}

\subsubsection{8. experiment}

\subsubsection{9. experiment}




\subsection{Otestování nejlepšího modelu}

\subsection{Překlad mezi více jazyky}


\chapter{Závěr}
Práce popisuje komponenty potřebné pro vytvoření překladového systému s pomocí neuronových sítí. Za pomocí těchto komponent byl vytvořen systém realizovaný pythoním balíčkem \emph{nmt}. Byly zvoleny trénovací a testovací data s kterými byl tento systém otestován. Hodnotícím kritériem je skore BLEU, které pro testovací dataset newtest2017 vyšlo 2.03. V porovnání se skóre 14.0, které vyšlo pro systém natrénovaný nástrojem Moses to není velmi úspěšný výsledek. Systém bude potřeba rozšířit a vylepšit, tak aby fungoval výrazně lépe.

Je několik vhodných rozšíření systému, které by mohly pomoci dosáhnout lepších výsledků. Použití více, případně i větších, vrstev LSTM. Na místo invertování vstupů enkodéru, je možné první vrstvu enkodéru udělat obousměrnou (viz \ref{subsection:optimization}). Dále by bylo vhodné přidat paprskové vyhledávání na místo hladového pro určování slov při generování. Dosavadní implementace systému vykazuje velký problém s neznámými (\emph{UNK}) slovy. Tento problém by se mohl vyřešit pomocí tzv. \emph{attention} mechanismu a přenášení neznámých slov v nezměněné podobě do výsledného překladu.


\todo{attention}
\todo{knihovna neni nutne specificka pro nmt, ale zavisla na datech, upravil bych api na vic genericky a zkusil natrenovat chatbota}
\todo{ze jsem se hodne zameroval na experimenty a data, ale chtel bych vylepsit dekompozici knihovny a jeji api a distribuovat na PyPI}
